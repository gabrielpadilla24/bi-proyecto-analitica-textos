# Etapa 1 - Proyecto Inteligencia de Negocios  
### Grupo 27  
**Autores:** Gabriel Padilla, Juan Pablo Rivera  

---

## Secci√≥n 1. (20%) Documentaci√≥n del proceso de aprendizaje autom√°tico  

Para esta secci√≥n se complet√≥ el **Canvas de Aprendizaje Autom√°tico** siguiendo la adaptaci√≥n de *OWNML MACHINE LEARNING CANVAS* (CRISP-ML(Q): The ML Lifecycle Process).  

üìé **Canvas del proyecto (PDF):**  
[Descargar Canvas de Aprendizaje Autom√°tico](./canvas_model.pdf)

---


## Secci√≥n 2. (20%) Entendimiento y preparaci√≥n de los datos

En esta etapa se realiz√≥ un **perfilamiento de los datos** con el fin de analizar su calidad, consistencia y completitud. El proceso incluy√≥ las siguientes tareas:

1. **Exploraci√≥n inicial de los datos**  
   - Identificaci√≥n de columnas relevantes para el problema de clasificaci√≥n de texto.  
   - Revisi√≥n de valores nulos, duplicados y distribuci√≥n de las variables.  
   - An√°lisis descriptivo para entender la representatividad de cada clase.

2. **Calidad de los datos**  
   - Se verific√≥ la existencia de inconsistencias en los registros (ejemplo: textos vac√≠os o mal codificados).  
   - Se eliminaron valores duplicados y se estandarizaron entradas inconsistentes.  
   - Se revis√≥ la distribuci√≥n de las clases para evaluar si exist√≠a desbalance.

3. **Tratamiento de los datos**  
   - **Limpieza de texto:** conversi√≥n a min√∫sculas, eliminaci√≥n de caracteres especiales y stopwords.  
   - **Tokenizaci√≥n:** segmentaci√≥n del texto en palabras para facilitar el an√°lisis.  
   - **Lematizaci√≥n/Stemming:** reducci√≥n de las palabras a su forma ra√≠z para mejorar la representaci√≥n.  
   - **Vectorizaci√≥n:** uso de representaciones como *TF-IDF* para transformar el texto en vectores num√©ricos.  

4. **Transformaciones aplicadas seg√∫n los algoritmos seleccionados**  
   - Para **Naive Bayes Multinomial**, se emple√≥ TF-IDF como representaci√≥n de texto, dado que este modelo se beneficia de frecuencias normalizadas.  
   - Para **Regresi√≥n Log√≠stica** y **SVM**, tambi√©n se utiliz√≥ TF-IDF, asegurando que las variables tuvieran la misma escala para optimizar la convergencia y precisi√≥n.  

5. **Preparaci√≥n final de los datos**  
   - Divisi√≥n del dataset en **conjunto de entrenamiento y prueba** con una proporci√≥n de 80/20.  
   - Validaci√≥n cruzada en la etapa de modelado para asegurar generalizaci√≥n.  
   - Exportaci√≥n de los datos procesados para su uso en los diferentes algoritmos.  

En conclusi√≥n, el proceso de **entendimiento y preparaci√≥n de los datos** permiti√≥ garantizar que la informaci√≥n utilizada en el modelado fuera **limpia, balanceada y transformada** adecuadamente, alineada con las t√©cnicas y algoritmos definidos para resolver el problema planteado.

---

## Secci√≥n 3. (20%) Modelado y selecci√≥n del modelo final

En esta etapa se entrenaron y evaluaron tres algoritmos de clasificaci√≥n de texto: **Naive Bayes Multinomial**, **Regresi√≥n Log√≠stica** y **SVM (Support Vector Machine)**. El proceso incluy√≥ los siguientes pasos:

1. **Definici√≥n de los modelos a entrenar**
   - **Naive Bayes Multinomial (NB):** Modelo probabil√≠stico simple basado en la independencia condicional de las palabras.  
   - **Regresi√≥n Log√≠stica (LR):** Modelo lineal discriminativo utilizado para clasificaci√≥n binaria y multiclase, optimizado mediante gradiente descendente.  
   - **SVM (Support Vector Machine):** Modelo que busca un hiperplano √≥ptimo para separar las clases maximizando el margen entre ellas.

2. **Pipeline de entrenamiento**
   - Transformaci√≥n de texto mediante **TF-IDF** para todos los modelos.  
   - Divisi√≥n del dataset en **conjunto de entrenamiento (80%) y prueba (20%)**.  
   - Validaci√≥n cruzada para evaluar estabilidad del rendimiento.  
   - Optimizaci√≥n de hiperpar√°metros en Regresi√≥n Log√≠stica y SVM.  

3. **Resultados obtenidos (m√©trica F1-macro)**

| Modelo               | F1-macro |
|-----------------------|----------|
| Naive Bayes          | 0.9353   |
| Regresi√≥n Log√≠stica  | 0.9577   |
| SVM                  | 0.9631   |

4. **Interpretaci√≥n de resultados**
   - **Naive Bayes:** Alcanz√≥ un rendimiento aceptable (F1 ‚âà 0.93), aunque con limitaciones en clases minoritarias.  
   - **Regresi√≥n Log√≠stica:** Mejor√≥ notablemente el rendimiento (F1 ‚âà 0.96), mostrando balance entre precisi√≥n y recall.  
   - **SVM:** Obtuvo el mejor desempe√±o (F1 ‚âà 0.963), con mayor capacidad para distinguir entre clases y reduciendo errores en la matriz de confusi√≥n.

5. **Selecci√≥n del modelo final**
   - Se seleccion√≥ **SVM (Support Vector Machine)** como el modelo final debido a:
     - Su mayor **F1-macro**.  
     - Balance √≥ptimo entre precisi√≥n y recall.  
     - Mejor generalizaci√≥n y robustez frente a casos ambiguos.  

Este modelo ser√° utilizado en las siguientes etapas del proyecto para la implementaci√≥n y validaci√≥n final.

### Contribuci√≥n de los integrantes
- **Naive Bayes Multinomial:** desarrollado por **Juan Pablo Rivera**.  
- **Regresi√≥n Log√≠stica:** desarrollada por **Gabriel Padilla**.  
- **SVM:** implementado y ajustado en conjunto por **Gabriel Padilla y Juan Pablo Rivera**.
